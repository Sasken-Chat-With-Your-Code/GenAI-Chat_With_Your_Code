# GenAI-Chat_With_Your_Code

## ğŸ–¥ï¸ Local Model Approach
This version of Chat With Code runs entirely on your local machine using Qwen 1.5-0.5B-Chat model for understanding and answering questions about a C/C++ codebase. It leverages FastAPI for serving the backend, FAISS for efficient vector-based retrieval of code chunks, and implements a RAG (Retrieval-Augmented Generation) pipeline to provide context-aware, accurate responses.

This setup ensures:  
- **Data Privacy** â€“ All code and queries remain within your local environment.  
- **Self-Contained Deployment** â€“ Packaged as a FastAPI service, allowing API-style interaction without requiring users to directly manage the model.  

## ğŸ”‘ Key Features  

- **Privacy-Preserving** â€“ No dependency on third-party APIs; runs locally or on a private server.  
- **API-First Design** â€“ Encapsulates Qwen inside a FastAPI service for easy integration via REST endpoints.  
- **Plug-and-Play Deployment** â€“ Works like a hosted API but fully self-hosted.  
- **High-Performance Retrieval** â€“ Uses **FAISS** for scalable semantic search across large codebases.  
- **Semantic Embeddings** â€“ Employs **SentenceTransformers (all-MiniLM-L6-v2)** for effective code/document representation.  
- **Context-Aware Responses** â€“ RAG pipeline enriches responses with relevant code snippets.  
- **Extensible Architecture** â€“ Built with FastAPI, supporting future extensions (auth, monitoring, logging, etc.).  


---

## ğŸ› ï¸ Tech Stack  

- **Programming Language** â€“ Python (backend), C/C++ (knowledge base)  
- **Backend Framework** â€“ [FastAPI](https://fastapi.tiangolo.com/)  
- **Vector Database** â€“ [FAISS](https://github.com/facebookresearch/faiss)  
- **Embedding Model** â€“ [SentenceTransformers](https://www.sbert.net/) (`all-MiniLM-L6-v2`)  
- **Local LLM** â€“ [Qwen 1.5-0.5B-Chat](https://huggingface.co/Qwen)  
- **Environment** â€“ `venv` or `conda` for dependency management  
- **Version Control** â€“ Git & GitHub  

---

## â–¶ï¸ Running the Project  

### Run Backend (FastAPI)  
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

### Run Frontend
cd frontend

npm install

npm run dev


---

#### Author - Shreya C Bharadwaj

