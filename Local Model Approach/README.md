# GenAI-Chat_With_Your_Code

### Local Model Approach
This version of Chat With Code runs entirely on your local machine using Qwen 1.5-0.5B-Chat model for understanding and answering questions about a C/C++ codebase. It leverages FastAPI for serving the backend, FAISS for efficient vector-based retrieval of code chunks, and implements a RAG (Retrieval-Augmented Generation) pipeline to provide context-aware, accurate responses.




### Credits

This project was developed collaboratively:

- **Shreya C Bharadwaj** – Local Model RAG pipeline, Qwen model integration, FAISS setup, FastAPI backend  
- **[Manasa Likhith]** – API approach (elaborate)  
- **[Laasya Siva Vaishali]** – Frontend using react and tailwind (elaborate)


